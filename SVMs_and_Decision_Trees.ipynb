{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstxMargaryan/ML/blob/main/SVMs_and_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a1493a-9460-4ff5-95b5-0c529a4bbda8",
      "metadata": {
        "id": "e1a1493a-9460-4ff5-95b5-0c529a4bbda8"
      },
      "source": [
        "## SVMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b9d4aeb-a184-4045-acf8-6afdd5bd482e",
      "metadata": {
        "id": "7b9d4aeb-a184-4045-acf8-6afdd5bd482e"
      },
      "source": [
        "### Homework: Implement a Linear SVM (Classification)\n",
        "\n",
        "In this assignment, you will implement a **linear Support Vector Machine (SVM)** for binary classification.  \n",
        "You are given the class skeleton below; your task is to complete the `fit` and `predict` methods.  \n",
        "Assume labels are binary and convert them internally to \\(\\{-1, +1\\}\\). The goal is to correctly learn the weight vector \\(w\\) and bias \\(b\\) and use them to make predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a159485-357a-4318-bb03-277c5589f137",
      "metadata": {
        "id": "4a159485-357a-4318-bb03-277c5589f137"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class SVM:\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y_signed = np.where(y<=0, -1, 1)\n",
        "\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "          for i in range(n_samples):\n",
        "            margin = y_signed[i] * (np.dot(self.w, X[i]) + self.b)\n",
        "\n",
        "            if margin >= 1:\n",
        "              self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "            else:\n",
        "              self.w -= self.lr * (2 * self.lambda_param * self.w - y_signed[i] * X[i])\n",
        "              self.b -= self.lr * (-y_signed[i])\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = np.dot(X, self.w) + self.b\n",
        "        y_pred = np.where(scores >= 0, 1, -1)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f570ef2-bd48-4667-8d94-6fd2a49e7493",
      "metadata": {
        "id": "4f570ef2-bd48-4667-8d94-6fd2a49e7493"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab8c610c-d8dc-4255-89ad-373579214030",
      "metadata": {
        "id": "ab8c610c-d8dc-4255-89ad-373579214030"
      },
      "source": [
        "### Homework: Implement a Decision Tree Classifier\n",
        "\n",
        "In this assignment, you will implement a **basic decision tree classifier from scratch**.  \n",
        "The tree should use the **Gini impurity** to choose splits and should grow recursively.\n",
        "\n",
        "Stop splitting when a maximum depth is reached **or** when a node contains too few samples.  \n",
        "You do **not** need to implement preprocessing, feature encoding, or pruning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8969adab-4371-4d2d-92a5-49326c64828b",
      "metadata": {
        "id": "8969adab-4371-4d2d-92a5-49326c64828b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=3, min_size=5):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "        self.tree = None\n",
        "\n",
        "    def gini(self, groups, classes):\n",
        "      \"\"\"Compute Gini impurity for a split.\"\"\"\n",
        "      n_instances = sum(len(group) for group in groups)\n",
        "      if n_instances == 0:\n",
        "        return 0.0\n",
        "\n",
        "      gini = 0.0\n",
        "      for group in groups:\n",
        "        size = len(group)\n",
        "        if size == 0:\n",
        "          continue\n",
        "\n",
        "        score = 0.00\n",
        "        labels = group[:, -1]\n",
        "        for c in classes:\n",
        "          p = np.sum(labels == c) / size\n",
        "          score += p * p\n",
        "        gini += (1-score) * (size / n_instances)\n",
        "      return gini\n",
        "\n",
        "    def split_dataset(self, index, value, dataset):\n",
        "      \"\"\"Split dataset based on feature index and threshold value.\"\"\"\n",
        "      left = dataset[dataset[:, index] < value]\n",
        "      right = dataset[dataset[:, index] >= value]\n",
        "      return left, right\n",
        "\n",
        "    def best_split(self, dataset):\n",
        "      \"\"\"Find the best split for a dataset.\"\"\"\n",
        "      classes = np.unique(dataset[:, -1])\n",
        "      n_features = dataset.shape[1] - 1\n",
        "\n",
        "      best_index, best_value, best_score, best_groups = None, None, float(\"inf\"), None\n",
        "\n",
        "      for index in range(n_features):\n",
        "        for row in dataset:\n",
        "          value = row[index]\n",
        "          groups = self.split_dataset(index, value, dataset)\n",
        "          gini = self.gini(groups, classes)\n",
        "          if gini < best_score:\n",
        "            best_index, best_value, best_score, best_groups = index, value, gini, groups\n",
        "\n",
        "      return {\"index\": best_index, \"value\": best_value, \"groups\": best_groups}\n",
        "\n",
        "    def to_terminal(self, group):\n",
        "      \"\"\"Create a terminal node (most common class).\"\"\"\n",
        "      labels = group[:, -1]\n",
        "      values, counts = np.unique(labels, return_counts=True)\n",
        "      return values[np.argmax(counts)]\n",
        "\n",
        "    def build_tree(self, node, depth):\n",
        "      \"\"\"Recursively build the decision tree.\"\"\"\n",
        "      left, right = node['groups']\n",
        "      del node['groups']\n",
        "\n",
        "      if len(left) == 0 or len(right) == 0:\n",
        "        node['left'] = node['right'] = self.to_terminal(np.vstack((left, right)))\n",
        "        return\n",
        "\n",
        "      if depth >= self.max_depth:\n",
        "        node['left'], node['right'] = self.to_terminal(left), self.to_terminal(right)\n",
        "        return\n",
        "\n",
        "      if len(left) <= self.min_size:\n",
        "        node['left'] = self.to_terminal(left)\n",
        "      else:\n",
        "        node['left'] = self.best_split(left)\n",
        "        self.build_tree(node['left'], depth + 1)\n",
        "\n",
        "      if len(right) <= self.min_size:\n",
        "        node['right'] = self.to_terminal(right)\n",
        "      else:\n",
        "        node['right'] = self.best_split(right)\n",
        "        self.build_tree(node['right'], depth + 1)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build the decision tree.\"\"\"\n",
        "        dataset = np.column_stack((X, y))\n",
        "        root = self.best_split(dataset)\n",
        "        self.build_tree(root, depth=1)\n",
        "        self.tree = root\n",
        "\n",
        "    def predict_one(self, node, x):\n",
        "      \"\"\"Predict class for a single sample.\"\"\"\n",
        "      if not isinstance(node, dict):\n",
        "        return node\n",
        "      if x[node['index']] < node['value']:\n",
        "        return self.predict_one(node['left'], x)\n",
        "      else:\n",
        "        return self.predict_one(node['right'], x)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict classes for multiple samples.\"\"\"\n",
        "        return np.array([self.predict_one(self.tree, x) for x in X])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}